# -*- coding: utf-8 -*-
"""untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/sreeyagalla/43caf6891656a1507b28ef01950b8984/untitled26.ipynb

Data Selection and pre-processing
"""

import pandas as pd
df=pd.read_csv('/content/diabetes.csv')

#cheching for null values
df.isnull().sum()

#checking for duplicates
df.duplicated().sum()

df.info()

#filling the 0's in the data as these parameters cannot be 0
new_df=df.copy()
new_df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']]=df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,pd.NA)

#now searching for missing values
new_df.isnull().sum()

#replacing the missing values with the right parameters
new_df['Glucose'].fillna(new_df['Glucose'].mean(),inplace=True)
new_df['BloodPressure'].fillna(new_df['BloodPressure'].mean(),inplace=True)
new_df['SkinThickness'].fillna(new_df['SkinThickness'].mean(),inplace=True)
new_df['Insulin'].fillna(new_df['Insulin'].mean(),inplace=True)
new_df['BMI'].fillna(new_df['BMI'].mean(),inplace=True)

new_df.isnull().sum()

#Finding count of people having diabetes and not having diabetes
new_df['Outcome'].value_counts()

#splitting the data as such features are taken into variable x(independent variable) and outcome values are taken into y (depended variable)
y=new_df['Outcome']
x=new_df.drop('Outcome',axis=1)

from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 42, stratify = y)

"""Exploratory Data Analysis (EDA) and Feature Selection"""

import seaborn as sns
import matplotlib.pyplot as plt
#Distribution plots
sns.pairplot(data=df,hue='Outcome')
plt.show()

# Correlation Analysis
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Feature Selection
# Univariate Feature Selection (example with ANOVA)
#Finding the relevant features for modeling
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=5)
X_selected = selector.fit_transform(df.drop('Outcome', axis=1), df['Outcome'])
selected_features = df.drop('Outcome', axis=1).columns[selector.get_support()]

print("Selected Features:", selected_features)

"""Model Implementation and Baseline Evaluation

svm
"""

from sklearn.svm import SVC
svm_model = SVC(kernel="linear")

svm_model.fit(X_test, y_test)
y_pred_svm = svm_model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm, recall_svm, f1_svm, _ = precision_recall_fscore_support(y_test, y_pred_svm, average='weighted')

# classification report for SVM
print(classification_report(y_test, y_pred_svm))

# Print the evaluation metrics
print("Accuracy:", accuracy_svm)
print("Precision:", precision_svm)
print("Recall:", recall_svm)
print("F1 Score:", f1_svm)

# plotting counfusion metric for SVM
svm_cm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(svm_cm, annot=True)
plt.show()

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

rf_model.fit(X_test, y_test)
y_pred_rf = rf_model.predict(X_test)

# Getting all accuracy socres for Random Forest
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf, recall_rf, f1_rf, _ = precision_recall_fscore_support(y_test, y_pred_rf, average='weighted')

# classification report for Random Forest
print(classification_report(y_test, y_pred_rf))

# Print the evaluation metrics
print("Accuracy:", accuracy_rf)
print("Precision:", precision_rf)
print("Recall:", recall_rf)
print("F1 Score:", f1_rf)

# plotting counfusion metric for Random Forest
rf_cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(rf_cm, annot=True)
plt.show()

"""KNN (K-Nearest Neighbours)"""

from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier(n_neighbors=5)

# Scaling our dataset to use it for KNN
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn_model.fit(X_train_scaled, y_train)

y_pred_knn = knn_model.predict(X_test_scaled)

# Getting all accuracy socres for KNN
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn, recall_knn, f1_knn, _ = precision_recall_fscore_support(y_test, y_pred_knn, average='weighted')

# classification report for KNN
print(classification_report(y_test, y_pred_knn))

# Print the evaluation metrics
print("Accuracy:", accuracy_knn)
print("Precision:", precision_knn)
print("Recall:", recall_knn)
print("F1 Score:", f1_knn)

# plotting counfusion metric for KNN
knn_cm = confusion_matrix(y_test, y_pred_knn)
sns.heatmap(knn_cm, annot=True)
plt.show()

"""Hyperparameter Tuning

SVM Grid Search
"""

# SVM
svm_param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf']}
svm_grid = GridSearchCV(SVC(), svm_param_grid, refit=True, verbose=3)
svm_grid.fit(X_train, y_train)
print("\nBest Parameters for SVM:", svm_grid.best_params_)
svm_best_model = svm_grid.best_estimator_
y_pred_svm_tuned = svm_best_model.predict(X_test)
accuracy_svm_tuned = accuracy_score(y_test, y_pred_svm_tuned)
print("Support Vector Machine Tuned Accuracy:", accuracy_svm_tuned)

# Re-evaluate models with tuned hyperparameters

# SVM
svm_best_model.fit(X_train, y_train)
y_pred_svm_tuned = svm_best_model.predict(X_test)
accuracy_svm_tuned = accuracy_score(y_test, y_pred_svm_tuned)
print("\nSupport Vector Machine Tuned Accuracy:", accuracy_svm_tuned)

# Random Forest
rf_param_grid = {'n_estimators': [50, 100, 200, 300], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, refit=True, verbose=3)
rf_grid.fit(X_train, y_train)
print("\nBest Parameters for Random Forest:", rf_grid.best_params_)
rf_best_model = rf_grid.best_estimator_
y_pred_rf_tuned = rf_best_model.predict(X_test)
accuracy_rf_tuned = accuracy_score(y_test, y_pred_rf_tuned)
print("Random Forest Tuned Accuracy:", accuracy_rf_tuned)

# Re-evaluate models with tuned hyperparameters
# Random Forest
rf_best_model.fit(X_train, y_train)
y_pred_rf_tuned = rf_best_model.predict(X_test)
accuracy_rf_tuned = accuracy_score(y_test, y_pred_rf_tuned)
print("Random Forest Tuned Accuracy:", accuracy_rf_tuned)

# KNN
knn_param_grid = {'n_neighbors': [3, 5, 7, 9, 11], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']}
knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid, refit=True, verbose=3)
knn_grid.fit(X_train, y_train)
print("\nBest Parameters for KNN:", knn_grid.best_params_)
knn_best_model = knn_grid.best_estimator_
y_pred_knn_tuned = knn_best_model.predict(X_test)
accuracy_knn_tuned = accuracy_score(y_test, y_pred_knn_tuned)
print("K-Nearest Neighbors Tuned Accuracy:", accuracy_knn_tuned)

# Re-evaluate models with tuned hyperparameters
# KNN
knn_best_model.fit(X_train, y_train)
y_pred_knn_tuned = knn_best_model.predict(X_test)
accuracy_knn_tuned = accuracy_score(y_test, y_pred_knn_tuned)
print("K-Nearest Neighbors Tuned Accuracy:", accuracy_knn_tuned)

"""Model Evaluation and Comparative Analysis"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Define function to evaluate models
def evaluate_model(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred)
    return accuracy, precision, recall, f1, roc_auc

# Evaluate models
accuracy_svm, precision_svm, recall_svm, f1_svm, roc_auc_svm = evaluate_model(y_test, y_pred_svm_tuned)
accuracy_rf, precision_rf, recall_rf, f1_rf, roc_auc_rf = evaluate_model(y_test, y_pred_rf_tuned)
accuracy_knn, precision_knn, recall_knn, f1_knn, roc_auc_knn = evaluate_model(y_test, y_pred_knn_tuned)

# Print evaluation metrics for SVM
print("Support Vector Machine:")
print("Accuracy:", accuracy_svm)
print("Precision:", precision_svm)
print("Recall:", recall_svm)
print("F1 Score:", f1_svm)
print("ROC-AUC:", roc_auc_svm)
print()

# Print evaluation metrics for Random Forest
print("Random Forest:")
print("Accuracy:", accuracy_rf)
print("Precision:", precision_rf)
print("Recall:", recall_rf)
print("F1 Score:", f1_rf)
print("ROC-AUC:", roc_auc_rf)
print()

# Print evaluation metrics for KNN
print("K-Nearest Neighbors:")
print("Accuracy:", accuracy_knn)
print("Precision:", precision_knn)
print("Recall:", recall_knn)
print("F1 Score:", f1_knn)
print("ROC-AUC:", roc_auc_knn)

import time
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# Function to evaluate models
def evaluate_model(model, X_train, y_train, X_test, y_test):
    start_time = time.time()
    model.fit(X_train, y_train)
    training_time = time.time() - start_time

    start_time = time.time()
    y_pred = model.predict(X_test)
    prediction_time = time.time() - start_time

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred)

    return accuracy, precision, recall, f1, roc_auc, training_time, prediction_time

# Initialize models
svm_model = SVC(kernel="linear")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
knn_model = KNeighborsClassifier(n_neighbors=5)

# Evaluate models
svm_accuracy, svm_precision, svm_recall, svm_f1, svm_roc_auc, svm_train_time, svm_pred_time = evaluate_model(svm_model, X_train, y_train, X_test, y_test)
rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc, rf_train_time, rf_pred_time = evaluate_model(rf_model, X_train, y_train, X_test, y_test)
knn_accuracy, knn_precision, knn_recall, knn_f1, knn_roc_auc, knn_train_time, knn_pred_time = evaluate_model(knn_model, X_train, y_train, X_test, y_test)

# Print evaluation results
print("Support Vector Machine:")
print("Accuracy:", svm_accuracy)
print("Precision:", svm_precision)
print("Recall:", svm_recall)
print("F1 Score:", svm_f1)
print("ROC-AUC:", svm_roc_auc)
print("Training Time:", svm_train_time)
print("Prediction Time:", svm_pred_time)
print()

print("Random Forest:")
print("Accuracy:", rf_accuracy)
print("Precision:", rf_precision)
print("Recall:", rf_recall)
print("F1 Score:", rf_f1)
print("ROC-AUC:", rf_roc_auc)
print("Training Time:", rf_train_time)
print("Prediction Time:", rf_pred_time)
print()

print("K-Nearest Neighbors:")
print("Accuracy:", knn_accuracy)
print("Precision:", knn_precision)
print("Recall:", knn_recall)
print("F1 Score:", knn_f1)
print("ROC-AUC:", knn_roc_auc)
print("Training Time:", knn_train_time)
print("Prediction Time:", knn_pred_time)

import matplotlib.pyplot as plt

# Evaluation metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']
svm_metrics = [svm_accuracy, svm_precision, svm_recall, svm_f1, svm_roc_auc]
rf_metrics = [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc]
knn_metrics = [knn_accuracy, knn_precision, knn_recall, knn_f1, knn_roc_auc]

# Plotting
plt.figure(figsize=(10, 6))

# Accuracy
plt.subplot(2, 3, 1)
plt.bar(['SVM', 'Random Forest', 'KNN'], [svm_accuracy, rf_accuracy, knn_accuracy], color=['blue', 'orange', 'green'])
plt.title('Accuracy')
plt.ylabel('Score')

# Precision
plt.subplot(2, 3, 2)
plt.bar(['SVM', 'Random Forest', 'KNN'], [svm_precision, rf_precision, knn_precision], color=['blue', 'orange', 'green'])
plt.title('Precision')
plt.ylabel('Score')

# Recall
plt.subplot(2, 3, 3)
plt.bar(['SVM', 'Random Forest', 'KNN'], [svm_recall, rf_recall, knn_recall], color=['blue', 'orange', 'green'])
plt.title('Recall')
plt.ylabel('Score')

# F1 Score
plt.subplot(2, 3, 4)
plt.bar(['SVM', 'Random Forest', 'KNN'], [svm_f1, rf_f1, knn_f1], color=['blue', 'orange', 'green'])
plt.title('F1 Score')
plt.ylabel('Score')

# ROC-AUC
plt.subplot(2, 3, 5)
plt.bar(['SVM', 'Random Forest', 'KNN'], [svm_roc_auc, rf_roc_auc, knn_roc_auc], color=['blue', 'orange', 'green'])
plt.title('ROC-AUC')
plt.ylabel('Score')

plt.tight_layout()
plt.show()

"""Conclusion and Recommendations

Based on the evaluation results of the three algorithms, we can summarize the findings and insights gained from the comparative analysis as follows:

**Accuracy:**

Random Forest performed the best in terms of accuracy with a score of approximately 78.6%, followed by SVM with around 70.8% accuracy. K-Nearest Neighbors (KNN) had the lowest accuracy of approximately 66.9%

**Precision:**

Random Forest also achieved the highest precision of approximately 72.3%, indicating its ability to correctly classify positive instances. SVM had a precision of 60%, while KNN had the lowest precision of 53.2%.

**Recall:**

Random Forest exhibited the highest recall of approximately 63%, indicating its ability to correctly identify true positive instances. SVM had a recall of 50%, and KNN had a recall of 46.3%

**ROC-AUC:**

Random Forest also outperformed in terms of ROC-AUC with a score of approximately 74.9%, indicating its ability to discriminate between positive and negative instances effectively. SVM had a ROC-AUC of 66%, and KNN had a ROC-AUC of 62.1%.

**Computational Efficiency:**
KNN had the shortest training time (0.005 seconds) and prediction time (0.023 seconds) among the three algorithms, indicating its simplicity and efficiency.
Random Forest had a moderate training time (0.598 seconds) and prediction time (0.042 seconds), while SVM had the longest training time (6.901 seconds) but very low prediction time (0.006 seconds).

In conclusion,


*   Random Forest appears to be the most balanced algorithm in terms of performance and computational efficiency. It achieved high accuracy, precision, recall, and F1 score, with reasonable computational overhead.
*  SVM, while achieving decent performance, may not be the best choice for large datasets due to its longer training time
*   KNN, while simple and efficient, may not be suitable for datasets with high dimensionality or noisy data.
"""